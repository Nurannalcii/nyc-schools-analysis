# ğŸ« NYC Schools Data Analysis

**NYC Schools Data Analysis** â€“ A Python-driven data pipeline for cleaning, transforming, and integrating NYC school datasets into a PostgreSQL database.

This project is part of the **Webeet.io Data Onboarding Program**.  
It demonstrates a complete **data pipeline** â€” from raw datasets to analysis and database integration using **Python and PostgreSQL**.

---

## ğŸ“… Project Structure Overview

| Task | Folder | Description |
|------|---------|-------------|
| **Day 1** | `incident_analysis/` | Google Sheets analysis of school incidents |
| **Day 2** | `school_directory_exploration/` | Data cleaning and demographic exploration |
| **Day 3** | `database_queries/` | SQL + Python integration for data exploration |
| **Day 4** | `database_population/` | SAT results integration and database population |
| **Day 5** | â€” | Final wrap-up and GitHub publishing |

âœ… **All stages are completed** â€” from raw data to structured analysis and database integration.  
ğŸ“Š This repository represents a full data pipeline project using Python, SQL, and PostgreSQL.

---

## ğŸš€ Project Overview

- Cleaned, analyzed, and integrated NYC school datasets using Python and SQL  
- Built clear and consistent data structures aligned with PostgreSQL schema  
- Documented findings, key insights, and workflows in Markdown for each task  
- Organized project by **task-based structure** (not by days)

---

## ğŸ“‚ Repository Structure

nyc-schools-analysis/
â”œâ”€â”€ incident_analysis/ # Day 1: School incident analysis (Google Sheets)
â”‚ â”œâ”€â”€ README.md # Findings, cleaning steps, insights
â”‚ â””â”€â”€ school-safety-report.csv # Source dataset used in analysis
â”‚
â”œâ”€â”€ school_directory_exploration/ # Day 2: School directory exploration
â”‚ â””â”€â”€ README.md # Data cleaning, Brooklyn filter, borough summaries
â”‚
â”œâ”€â”€ database_queries/ # Day 3: SQL exploration via Python
â”‚ â””â”€â”€ README.md # Queries, borough-level insights, ELL rates, etc.
â”‚
â”œâ”€â”€ database_population/ # Day 4: SAT results cleaning & DB integration
â”‚ â”œâ”€â”€ README.md # Schema design + load plan
â”‚ â”œâ”€â”€ cleaned_sat_results.csv # Cleaned SAT dataset
â”‚ â”œâ”€â”€ sat_modeling.ipynb # Cleaning + append-to-database logic (Python)
â”‚ â””â”€â”€ schema_notes.md # Table structure / FK notes
â”‚
â”œâ”€â”€ requirements.txt # Python environment (pandas, SQLAlchemy, etc.)
â””â”€â”€ README.md # Project overview (this file)

---

## ğŸ§  Tools & Technologies

| Library | Purpose / Why It Was Used |
|----------|----------------------------|
| **pandas** | To read CSV files, manipulate datasets as DataFrames, and execute SQL queries with `pd.read_sql()` |
| **numpy** | Provides numerical operations and supports Pandas in handling large numeric datasets |
| **sqlalchemy** | Enables Python-to-PostgreSQL connection via `create_engine` and allows SQL queries within Python |
| **psycopg2-binary** | PostgreSQL adapter used by SQLAlchemy to connect and execute queries |
| **matplotlib** | Used for data visualization â€” creating bar charts, line charts, and other plots (`plt.bar()`, `plt.plot()`) |
| **jupyter** | Required to run `.ipynb` notebooks and manage interactive data analysis workflows |
| **notebook** | Supports the Jupyter interface; ensures compatibility with JupyterLab and local notebook execution |

---

### ğŸ’¾ Installation  

To install all required dependencies, run:

```bash
pip install -r requirements.txt

---

ğŸ§¾ Key Deliverables
Clean and consistent datasets
SQL queries executed via Python
Data visualizations and insights
Final structured project ready for portfolio presentation


ğŸŒ Author
Nuran Nalci
ğŸ“ GitHub Repository
